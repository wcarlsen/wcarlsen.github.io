{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spaghetti  (blog)","text":""},{"location":"about/","title":"About me","text":"<p>I (Willi Carlsen) am 37 years old and I work as a Cloud Engineer at DFDS. I live in Copenhagen with my family. My spare time is spend on family, friends, fishing, self hosting, programming, Linux, Nix, Kubernetes, the command line, Open-Source software, cloud computing/infrastructure, sailing, photography and much more.</p> <p></p>"},{"location":"cv/","title":"CV","text":""},{"location":"cv/#resume","title":"Resume","text":"<p>I (Willi Carlsen) am 37 years old and work as a Senior Cloud Engineer at Veo Technologies, a company that strives to make sports technology available to the masses. I consider myself an AWS and Kubernetes expert, and I have spent the last ~3 years building multiple highly scalable Kubernetes clusters for video processing, running more than 2000 nodes at peak. Before that I used ~4 years supporting digital transformation at DFDS building a developer platform utilizing AWS, Kubernetes and Kafka.</p>"},{"location":"cv/#experience","title":"Experience","text":""},{"location":"cv/#cloud-engineer","title":"Cloud Engineer","text":""},{"location":"cv/#dfds-aug-25-present","title":"DFDS (Aug 25 - present)","text":"<p>Too early to say...</p>"},{"location":"cv/#senior-cloud-engineer","title":"Senior Cloud Engineer","text":""},{"location":"cv/#veo-technologies-aug-22-jul-25","title":"Veo Technologies (Aug 22 - Jul 25)","text":"<p>Building highly scalable Kubernetes clusters supporting video processing in AWS.</p>"},{"location":"cv/#site-reliability-engineer","title":"Site Reliability Engineer","text":""},{"location":"cv/#dfds-may-19-aug-22","title":"DFDS (May 19 - Aug 22)","text":"<p>Supporting digital transformation towards composable architecture and faster time to market via a developer platform utilizing AWS, Kubernetes and Kafka. The majority of the projects are open source on Github https://github.com/DFDS, go check them out.</p>"},{"location":"cv/#data-engineer","title":"Data Engineer","text":""},{"location":"cv/#dfds-jul-18-apr-19","title":"DFDS (Jul 18 - Apr 19)","text":"<p>Helped lowering technical debt by transforming PoC's into production ready maintainable solutions, handling and creating data pipelines, Cloud solution deployment, web development, introducing some of software developments best practices into the area of data science, reducing complexity, risks and future need for maintenance.</p>"},{"location":"cv/#growth-hackerdata-scientist","title":"Growth Hacker/Data scientist","text":""},{"location":"cv/#simplesite-feb-16-jun-18","title":"SimpleSite (Feb 16 - Jun 18)","text":"<p>Data crunching, web development and implementing and evaluating A/B testing at a large scale.</p>"},{"location":"cv/#education","title":"Education","text":""},{"location":"cv/#msc-in-physics","title":"MSc in Physics","text":""},{"location":"cv/#niels-bohr-institute-university-of-copenhagen-09-15","title":"Niels Bohr Institute University of Copenhagen (09 - 15)","text":"<p>An experiment trying, via optomechanical effects, to laser cool a nano-membrane to its ground state. The nano-membrane motion is caused by Brownian motion due to its finite temperature. It is exactly this vibrational noise we cooled away to glance at a macroscopic objects quantum behavior. The membrane was laser cooled from cryogenic temperature (4 K) down to 3 mK, yielding a 4% chance of observing the object in its ground state.</p>"},{"location":"cv/#skills","title":"Skills","text":"<ul> <li>Golang</li> <li>Python</li> <li>Terraform/OpenTofu</li> <li>Nix</li> <li>Kubernetes</li> <li>AWS</li> <li>GitOps practices</li> <li>Scripting and automation</li> <li>and much more</li> </ul>"},{"location":"cv/#other-things-that-im-proud-of","title":"Other things that I'm proud of","text":"<ul> <li>kubernetes/autoscaler contributor https://github.com/kubernetes/autoscaler/pull/6482</li> <li>loft/vcluster contributor https://github.com/loft-sh/vcluster/pull/318 and https://github.com/loft-sh/vcluster/pull/383</li> <li>Co-creator of Crossplane provider for Confluent Kafka (now archived) https://github.com/dfds/provider-confluent</li> <li>Certified Kubernetes Administrator (CKA) 2020-2022</li> <li>Recieved Niels Bohr Institute yearly teaching prize in 2013 the Jens Martin Knudsen teaching prize for the performance done in laboratory courses in Classical Mechanics 1 &amp; 2. Teaching students physics, laboratory routines, statistics and programming.</li> </ul>"},{"location":"cv/#about-me","title":"About me","text":"<p>See here.</p>"},{"location":"feeds/","title":"RSS feeds","text":"<p>Subscribe to the blog with a RSS reader using the following links.</p> <ul> <li>RSS Feed</li> <li>JSON Feed</li> </ul>"},{"location":"2025-02-25-intro-renovate/","title":"Automatic dependency updates (Renovate)","text":"<p>I've for a while now been running selfhosted Renovate at work for handling automatic dependency updates for my team and I can only recommend it. It's like Github's dependabot but on steroids and very simple to setup.</p> <p>Setup can be structured in two ways, I have implemented the latter.</p> <ul> <li>per repository - flexible but not very DRY (don't repeat yourself)</li> <li>centralised - not as flexible but very DRY</li> </ul> <p>All that is needed is a <code>config.js</code> file.</p> <pre><code>module.exports = {\n  branchPrefix: 'update/renovate/',\n  username: 'your-service-account-name',\n  onboarding: false,\n  requireConfig: 'optional',\n  platform: 'github',\n  repositories: [\n    'wcarlsen/repository0',\n    'wcarlsen/repository1',\n  ],\n  packageRules: [\n    {\n      matchUpdateTypes: [\n        'digest',\n        'lockFileMaintenance',\n        'patch',\n        'pin',\n      ],\n      minimumReleaseAge: '1 day',\n      automerge: false,\n      matchCurrentVersion: '!/(^0|alpha|beta)/',\n      dependencyDashboard: true,\n    },\n    {\n      matchUpdateTypes: [\n        'minor'\n      ],\n      minimumReleaseAge: '7 day',\n      automerge: false,\n      matchCurrentVersion: '!/(^0|alpha|beta)/',\n      dependencyDashboard: true,\n    },\n    {\n      matchUpdateTypes: [\n        'major'\n      ],\n      minimumReleaseAge: '14 day',\n      automerge: false,\n      dependencyDashboard: true,\n    },\n  ],\n};\n</code></pre> <p>and a Github action and service account PAT.</p> <pre><code>name: Renovate\non:\n  schedule:\n    - cron: \"15 2 * * 1-5\" # Every week day at 02.15\n  workflow_dispatch:\njobs:\n  renovate:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Self-hosted Renovate\n        uses: renovatebot/github-action@02f4fdeb479bbb229caa7ad82cb5e691c07e80b3 # v41.0.14\n        env:\n          LOG_LEVEL: ${{ vars.LOG_LEVEL || 'info' }}\n          RENOVATE_INTERNAL_CHECKS_FILTER: none\n        with:\n          configurationFile: config.js\n          token: ${{ secrets.RENOVATE_TOKEN }}\n</code></pre> <p>Local overwrites can be done in the repositories root with a <code>renovate.json</code>.</p> <pre><code>{\n  \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",\n  \"packageRules\": [\n    {\n      \"matchPackageNames\": [\"registry.k8s.io/autoscaling/cluster-autoscaler\"],\n      \"allowedVersions\": \"&lt;1.33.0\"\n    }\n  ]\n}\n</code></pre> <p>Enjoy those well deserved automatic dependency updates.</p> <p></p>","tags":["github","renovate","updates"]},{"location":"2025-03-01-run-and-debug-renovate-locally/","title":"Run and debug Renovate locally","text":"<p>Last I gave a quick introduction to Renovate and how to run it in centralised configuration. Today we will go over how to run Renovate locally for debugging and extending configuration purpose, which is very handy.</p> <pre><code>npx --yes --package renovate -- renovate --dry-run=full --token=\"GITHUB_TOKEN\" wcarlsen/repository0\n</code></pre> <p>This requires only a Github token and to change <code>LOG_LEVEL</code>, just set it as an environment variable to <code>DEBUG</code>.</p> <p>Now go customise your <code>config.js</code> or <code>renovate.json</code> config files to get the best out of Renovate.</p>","tags":["github","renovate","updates"]},{"location":"2025-04-03-docker-hub-rate-limits/","title":"Docker Hub rate limits","text":"<p>Docker Hub recently announced 10 pulls/hour for unauthenticated users. This has pretty significant impact in container orchestration, e.g. Kubernetes. I will not cover whether it is fair or not, but give credit to Docker Hub for its contributions to the community.</p> <p>So how does this rate limit impact Kubernetes?</p> <p>It can be hard to predict how many images will be pulled when a new node joins the cluster from an operator/administrator perspective.</p> <p>How could you solve it?</p> <p>We've opted for implementing a AWS ECR pull through cache. It is easy to setup and works like a charm.</p> <p>Where there any side effects?</p> <ol> <li> <p>All image references in manifests has to change from <code>nginx:latest</code> to <code>ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/docker-hub/nginx:latest</code> (don't use latest)</p> </li> <li> <p>Flux GitOps ImageUpdateAutomation breaks for CRD resources that reference images</p> </li> <li> <p>Renovate updates breaks because the cache doesn't have knowledge of new tags</p> </li> </ol> <p>I will try to cover possible solutions for above side effects in future posts.</p>","tags":["docker-hub","aws","kubernetes","renovate"]},{"location":"2025-04-09-vpa-flush-historical-data/","title":"Vertical pod autoscaler flush historical data","text":"<p>We recently had to roll out a new release of DCGM exporter, a tool that monitors Nvidia GPU performance and outputs metrics. It runs as a DaemonSet on all GPU Kubernetes nodes. With the new release there is a significant increase in memory resource consumption, normally this would be easy to handle through increasing resource requests and limits. But what happens if you decided to have Vertical Pod Autoscaler (VPA) manage resources through it's auto mode.</p>","tags":["kubernetes"]},{"location":"2025-04-09-vpa-flush-historical-data/#introduction-to-vertical-pod-autoscaler","title":"Introduction to Vertical Pod Autoscaler","text":"<p>Have you ever deployed a new and shiny thing, no matter if its custom or something off the shelf, and felt like choosing resource requests and limits was totally unqualified. This is where Vertical Pod Autoscaler comes into the picture, it can free users from setting or guessing resource requests and limits on containers in their pods and updating them if requirements changes.</p> <p>VPA can run in two modes recommendation or auto mode. Recommendation mode has a lot of value by it self by analysis current and historical resource usage, but requires you to manual changes to follow the recommended resources settings. Auto mode uses the recommendation, but can also adjust resources on the fly. This is great and has a lot of benefits among them to not waste resources on services that fluctuate and cannot scale horizontally.</p> <p>We run a lot services in VPA auto mode, among them the DCGM exporter.</p>","tags":["kubernetes"]},{"location":"2025-04-09-vpa-flush-historical-data/#roll-out-new-release-of-dcgm-exporter","title":"Roll out new release of DCGM exporter","text":"<p>We already knew from testing that the DCGM exporter had a significant increase in memory resource consumption, so we changed the <code>maxAllowed.memory</code> specification on the <code>VerticalPodAutoscaler</code> custom resource. The hope was that VPA would automatically adjust resources for the DCGM exporter rather quickly, but that didn't happen. DCGM exporter went into OOMKill crashlooping mode while the recommended memory from the VPA slowly crawled upwards. The OOmKill was expected but the slow adjustment from VPA was a surprise. There where probably many contributing factors, but the crashloop backoff didn't help.</p> <p>So how did we solve it?</p>","tags":["kubernetes"]},{"location":"2025-04-09-vpa-flush-historical-data/#flushing-vpa-historical-data","title":"Flushing VPA historical data","text":"<p>In the end we ended up deleting the appropiate <code>VPACheckpoint</code> resource and flushing memory on the VPA recommender component.</p> <pre><code>kubectl delete vpaceckpoint -n dcgm-exporter dcgm-exporter\nkubectl delete pod -n kube-system -l app=vpa-recommender\n</code></pre> <p>This almost immidiatly got the dcgm-exporter to the appropiate resources and out of OOMKill crashlooping.</p>","tags":["kubernetes"]},{"location":"2025-06-12-upgrade-al2023-learnings/","title":"Upgrade from AL2 to AL2023 learnings","text":"<p>Ever since AWS annouced that Amazon Linux 2023 (AL2023) AMI type is replacing Amazon Linux 2 (AL2), I have been excited about it. Mainly because of the cgroup v2 upgrade and the improved security with IMDSv2. To explain it quick</p> <ul> <li>cgroup v2 should provide more transparency when container sub-processes are OOM killed.</li> <li>IMDSv2 will block pods calling the metadata service on the nodes (getting an AWS context) due to a network hop limit.</li> </ul> <p>The AMI upgrade is needed for upgrading worker nodes on EKS from 1.32 to 1.33, since no AL2 AMI is build for 1.33.</p> <p>Upon testing we found a few things breaking, but nothing major. The AWS load balancer controller broke, but only needed the <code>--aws-vpc-id</code> and <code>--aws-region</code> flag set to work again. We ended up removing the spot-termination-exporter (supplying insight into spot-instance interruptions), since it realies heavily on the metadata service, which was now blocked. Sad, but we have lived without it before.</p> <p>We then went on to upgrading all clusters and worker nodes to version 1.33. The upgrade went smooth except for one thing that we overlooked. We rely on flux image-reflector-controller to scan container registries and that also uses the metadata service to use get context of the nodes. Luckily this was a fairly easy fix, where we ended up patching an IRSA role annotation to the image-reflector-controller ServiceAccount in following way.</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - gotk-components.yaml\n  - gotk-sync.yaml\npatches:\n  - patch: |\n      apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        name: image-reflector-controller\n        annotations:\n          eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/eks_CLUSTER_NAME_flux-image-reflector\n    target:\n      kind: ServiceAccount\n      name: image-reflector-controller\n</code></pre> <p>We are now enjoing AL2023 and are so far happy with the upgrade.</p>","tags":["aws","kubernetes"]},{"location":"2025-07-01-ebs-csi-and-al2023/","title":"EBS CSI driver and AL2023","text":"<p>After upgrading to Amazon Linux 2023 (AL2023) we started seeing errors from the aws-ebs-csi-driver running in our clusters.</p> <pre><code>ebs-plugin I0626 06:40:25.662215       1 main.go:154] \"Initializing metadata\"\nebs-plugin I0626 06:40:25.662374       1 metadata.go:66] \"Attempting to retrieve instance metadata from IMDS\"\nebs-plugin E0626 06:40:30.665263       1 metadata.go:72] \"Retrieving IMDS metadata failed\" err=\"could not get IMDS metadata: operation error ec2imds: GetInstanceIdentityDocument, canceled, context deadline exceeded\"\nebs-plugin I0626 06:40:30.665357       1 metadata.go:75] \"Attempting to retrieve instance metadata from Kubernetes API\"\n</code></pre> <p>This is due to AL2023 improved security ensuring features blocking pods from calling metadata service on the nodes due to a network hop limit of 1. The aws-ebs-csi-driver eventually falls back to using the Kubernetes API, but we are waiting ~5 seconds for the call to timeout. With the release of aws-ebs-csi-driver v1.45.0 they have implemented a flag (<code>--metadata-sources</code>) allowing us to set a priority order or choose a specific way of getting metadata. In our case it would be set to <code>\"kubernetes\"</code>.</p> <p>This should prevent above shown errors.</p>","tags":["aws","kubernetes"]},{"location":"2025-07-23-kubeconform/","title":"Client side validations of Kubernetes manifests","text":"<p>To be honest writing Kubernetes manifests can be tedius and it prone to misconfiguration. Of course it will in the end be validated server side, but we would like to avoid most errors before we hand off the manifests to the API server. This can be particular helpful when utilizing GitOps, since the changes will be consumed asynchronous. To achieve this will use the following tooling:</p> <ul> <li>pre-commit</li> <li>kustomize</li> <li>kubeconform</li> <li>CRDs catalog by Datree</li> <li>Github action pre-commit<sup>1</sup></li> </ul> <p>Let's start with <code>kustomize</code> and make sure that we can actually build our manifest bundle.</p> <pre><code>kustomize build path-to-kustomziation-file\n</code></pre> <p>We can now add this to <code>.pre-commit-config.yaml</code> file to the root of the project to have it run every time we commit.</p> <pre><code>repos:\n- repo: local\n  hooks:\n  - id: kustomize\n    name: validate kustmoizations\n    language: system\n    entry: kustomize\n    args:\n    - build\n    - path-to-kustomziation-file\n    always_run: true\n    pass_filenames: false\n</code></pre> <p>Now on to <code>kubeconform</code> for validating our manifests.</p> <pre><code>kubeconform -strict -skip CustomResourceDefinition,Kustomization \\\n  -kubernetes-version 1.33.0 \\\n  -schema-location default \\\n  -schema-location 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json' \\\n  path-to-your-manifests\n</code></pre> <p>We of course depend on the CRDs catalog having our CRs and them being updated, but it is relatively easy to contribute to the catalog see PRs #453 and #600.</p> <p>We can now also add this to our pre-commit config file like so.</p> <pre><code>repos:\n...\n- repo: local\n  hooks:\n  - id: kubeconform\n    name: validate kubernetes manifests\n    language: system\n    entry: kubeconform\n    args:\n    - -strict\n    - -kubernetes-version 1.33.0\n    - -skip\n    - CustomResourceDefinition,Kustomization\n    - -schema-location\n    - default\n    - -schema-location\n    - 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json'\n    files: ^path-to-your-manifests/.*\n</code></pre> <p>Using <code>pre-commit</code> is nice to validate your commits, but it requires everybody to install it and running <code>pre-commit install</code>. So to enforce above validations we can add a CI step in the form of a Github action.</p> <pre><code>name: Pre-commit\non:\n  - pull_request\njobs:\n  pre-commit:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-python@v5\n    - uses: alexellis/arkade-get@master\n      with:\n        kustomize: latest\n        kubeconform: latest\n    - uses: pre-commit/action@v3.0.1\n</code></pre> <p>This setup is not bullet proof, but it do add some extra confidence and it is very low effort to get going.</p> <ol> <li> <p>This action is in maintenance-only mode and you should support the project by using pre-commit.ci instead. But so that everyone can follow the other option is used.\u00a0\u21a9</p> </li> </ol>","tags":["kubernetes"]},{"location":"archive/2025/","title":"2025","text":""}]}