{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Spaghetti  (tech blog)","text":""},{"location":"about/","title":"About me","text":"<p>I (Willi Carlsen) am 37 years old and I work as a Cloud Engineer at DFDS. I live in Copenhagen with my family. My spare time is spend on family, friends, fishing, self hosting, programming, Linux, Nix, Kubernetes, the command line, Open-Source software, cloud computing/infrastructure, sailing, photography and much more.</p> <p></p>"},{"location":"cv/","title":"CV","text":""},{"location":"cv/#resume","title":"Resume","text":"<p>I (Willi Carlsen) am 37 years old and work as a Senior Cloud Engineer at DFDS, a big danish logistics company, where I support digital transformation via building a developer platform utilizing AWS, Kubernetes, Kafka and more. I consider myself an AWS and Kubernetes expert and is an active contributor in the opensource community surrounding it.</p>"},{"location":"cv/#experience","title":"Experience","text":""},{"location":"cv/#senior-cloud-engineer","title":"Senior Cloud Engineer","text":""},{"location":"cv/#dfds-aug-25-present","title":"DFDS (Aug 25 - present)","text":"<p>Developing and maintaining DFDS's platform setup, helping developers to fast and easy build and deploy. DFDS has been investing in Cloud and Kubernetes and currently runs cluster with almost ~3000 pods.</p>"},{"location":"cv/#senior-cloud-engineer_1","title":"Senior Cloud Engineer","text":""},{"location":"cv/#veo-technologies-aug-22-jul-25","title":"Veo Technologies (Aug 22 - Jul 25)","text":"<p>Build highly scalable Kubernetes clusters supporting video processing in AWS. Veo Technologies, is a company that strives to make sports technology available to the masses. I spent ~3 years building multiple highly scalable Kubernetes clusters for video processing, running more than ~2000 GPU nodes at peak every weekend.</p>"},{"location":"cv/#site-reliability-engineer","title":"Site Reliability Engineer","text":""},{"location":"cv/#dfds-may-19-aug-22","title":"DFDS (May 19 - Aug 22)","text":"<p>Supporting digital transformation towards composable architecture and faster time to market via a developer platform utilizing AWS, Kubernetes and Kafka. The majority of the projects are open source on Github https://github.com/DFDS, go check them out.</p>"},{"location":"cv/#data-engineer","title":"Data Engineer","text":""},{"location":"cv/#dfds-jul-18-apr-19","title":"DFDS (Jul 18 - Apr 19)","text":"<p>Helped lowering technical debt by transforming PoC's into production ready maintainable solutions, handling and creating data pipelines, Cloud solution deployment, web development, introducing some of software developments best practices into the area of data science, reducing complexity, risks and future need for maintenance.</p>"},{"location":"cv/#growth-hackerdata-scientist","title":"Growth Hacker/Data scientist","text":""},{"location":"cv/#simplesite-feb-16-jun-18","title":"SimpleSite (Feb 16 - Jun 18)","text":"<p>Data crunching, web development and implementing and evaluating A/B testing at a large scale.</p>"},{"location":"cv/#education","title":"Education","text":""},{"location":"cv/#msc-in-physics","title":"MSc in Physics","text":""},{"location":"cv/#niels-bohr-institute-university-of-copenhagen-09-15","title":"Niels Bohr Institute University of Copenhagen (09 - 15)","text":"<p>An experiment trying, via optomechanical effects, to laser cool a nano-membrane to its ground state. The nano-membrane motion is caused by Brownian motion due to its finite temperature. It is exactly this vibrational noise we cooled away to glance at a macroscopic objects quantum behavior. The membrane was laser cooled from cryogenic temperature (4 K) down to 3 mK, yielding a 4% chance of observing the object in its ground state.</p>"},{"location":"cv/#skills","title":"Skills","text":"<ul> <li>Golang</li> <li>Python</li> <li>Terraform/OpenTofu</li> <li>Nix</li> <li>Kubernetes</li> <li>AWS</li> <li>GitOps practices</li> <li>Scripting and automation</li> <li>and much more</li> </ul>"},{"location":"cv/#other-things-that-im-proud-of","title":"Other things that I'm proud of","text":"<ul> <li>1Password/load-secrets-action contributor</li> <li>Home-manager maintainer and contributor</li> <li>Nixpkgs maintainer and contributor</li> <li>kubernetes/autoscaler contributor</li> <li>loft/vcluster contributor</li> <li>Co-creator of Crossplane provider for Confluent Kafka (now archived)</li> <li>Certified Kubernetes Administrator (CKA) 2020</li> <li>Recieved Niels Bohr Institute yearly teaching prize in 2013 the Jens Martin Knudsen teaching prize for the performance done in laboratory courses in Classical Mechanics 1 &amp; 2. Teaching students physics, laboratory routines, statistics and programming.</li> </ul>"},{"location":"cv/#about-me","title":"About me","text":"<p>See here.</p>"},{"location":"feeds/","title":"RSS feeds","text":"<p>Subscribe to the blog with a RSS reader using the following links.</p> <ul> <li>RSS Feed</li> <li>JSON Feed</li> </ul>"},{"location":"2025-02-25-intro-renovate/","title":"Automatic dependency updates (Renovate)","text":"<p>I've for a while now been running selfhosted Renovate at work for handling automatic dependency updates for my team and I can only recommend it. It's like Github's dependabot but on steroids and very simple to setup.</p> <p>Setup can be structured in two ways, I have implemented the latter.</p> <ul> <li>per repository - flexible but not very DRY (don't repeat yourself)</li> <li>centralised - not as flexible but very DRY</li> </ul> <p>All that is needed is a <code>config.js</code> file.</p> <pre><code>module.exports = {\n  branchPrefix: 'update/renovate/',\n  username: 'your-service-account-name',\n  onboarding: false,\n  requireConfig: 'optional',\n  platform: 'github',\n  repositories: [\n    'wcarlsen/repository0',\n    'wcarlsen/repository1',\n  ],\n  packageRules: [\n    {\n      matchUpdateTypes: [\n        'digest',\n        'lockFileMaintenance',\n        'patch',\n        'pin',\n      ],\n      minimumReleaseAge: '1 day',\n      automerge: false,\n      matchCurrentVersion: '!/(^0|alpha|beta)/',\n      dependencyDashboard: true,\n    },\n    {\n      matchUpdateTypes: [\n        'minor'\n      ],\n      minimumReleaseAge: '7 day',\n      automerge: false,\n      matchCurrentVersion: '!/(^0|alpha|beta)/',\n      dependencyDashboard: true,\n    },\n    {\n      matchUpdateTypes: [\n        'major'\n      ],\n      minimumReleaseAge: '14 day',\n      automerge: false,\n      dependencyDashboard: true,\n    },\n  ],\n};\n</code></pre> <p>and a Github action and service account PAT.</p> <pre><code>name: Renovate\non:\n  schedule:\n    - cron: \"15 2 * * 1-5\" # Every week day at 02.15\n  workflow_dispatch:\njobs:\n  renovate:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Self-hosted Renovate\n        uses: renovatebot/github-action@02f4fdeb479bbb229caa7ad82cb5e691c07e80b3 # v41.0.14\n        env:\n          LOG_LEVEL: ${{ vars.LOG_LEVEL || 'info' }}\n          RENOVATE_INTERNAL_CHECKS_FILTER: none\n        with:\n          configurationFile: config.js\n          token: ${{ secrets.RENOVATE_TOKEN }}\n</code></pre> <p>Local overwrites can be done in the repositories root with a <code>renovate.json</code>.</p> <pre><code>{\n  \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",\n  \"packageRules\": [\n    {\n      \"matchPackageNames\": [\"registry.k8s.io/autoscaling/cluster-autoscaler\"],\n      \"allowedVersions\": \"&lt;1.33.0\"\n    }\n  ]\n}\n</code></pre> <p>Enjoy those well deserved automatic dependency updates.</p> <p></p>","tags":["github","renovate","updates"]},{"location":"2025-03-01-run-and-debug-renovate-locally/","title":"Run and debug Renovate locally","text":"<p>Last I gave a quick introduction to Renovate and how to run it in centralised configuration. Today we will go over how to run Renovate locally for debugging and extending configuration purpose, which is very handy.</p> <pre><code>npx --yes --package renovate -- renovate --dry-run=full --token=\"GITHUB_TOKEN\" wcarlsen/repository0\n</code></pre> <p>This requires only a Github token and to change <code>LOG_LEVEL</code>, just set it as an environment variable to <code>DEBUG</code>.</p> <p>Now go customise your <code>config.js</code> or <code>renovate.json</code> config files to get the best out of Renovate.</p>","tags":["github","renovate","updates"]},{"location":"2025-04-03-docker-hub-rate-limits/","title":"Docker Hub rate limits","text":"<p>Docker Hub recently announced 10 pulls/hour for unauthenticated users. This has pretty significant impact in container orchestration, e.g. Kubernetes. I will not cover whether it is fair or not, but give credit to Docker Hub for its contributions to the community.</p> <p>So how does this rate limit impact Kubernetes?</p> <p>It can be hard to predict how many images will be pulled when a new node joins the cluster from an operator/administrator perspective.</p> <p>How could you solve it?</p> <p>We've opted for implementing a AWS ECR pull through cache. It is easy to setup and works like a charm.</p> <p>Where there any side effects?</p> <ol> <li> <p>All image references in manifests has to change from <code>nginx:latest</code> to <code>ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/docker-hub/nginx:latest</code> (don't use latest)</p> </li> <li> <p>Flux GitOps ImageUpdateAutomation breaks for CRD resources that reference images</p> </li> <li> <p>Renovate updates breaks because the cache doesn't have knowledge of new tags</p> </li> </ol> <p>I will try to cover possible solutions for above side effects in future posts.</p>","tags":["docker-hub","aws","kubernetes","renovate"]},{"location":"2025-04-09-vpa-flush-historical-data/","title":"Vertical pod autoscaler flush historical data","text":"<p>We recently had to roll out a new release of DCGM exporter, a tool that monitors Nvidia GPU performance and outputs metrics. It runs as a DaemonSet on all GPU Kubernetes nodes. With the new release there is a significant increase in memory resource consumption, normally this would be easy to handle through increasing resource requests and limits. But what happens if you decided to have Vertical Pod Autoscaler (VPA) manage resources through it's auto mode.</p>","tags":["kubernetes"]},{"location":"2025-04-09-vpa-flush-historical-data/#introduction-to-vertical-pod-autoscaler","title":"Introduction to Vertical Pod Autoscaler","text":"<p>Have you ever deployed a new and shiny thing, no matter if its custom or something off the shelf, and felt like choosing resource requests and limits was totally unqualified. This is where Vertical Pod Autoscaler comes into the picture, it can free users from setting or guessing resource requests and limits on containers in their pods and updating them if requirements changes.</p> <p>VPA can run in two modes recommendation or auto mode. Recommendation mode has a lot of value by it self by analysis current and historical resource usage, but requires you to manual changes to follow the recommended resources settings. Auto mode uses the recommendation, but can also adjust resources on the fly. This is great and has a lot of benefits among them to not waste resources on services that fluctuate and cannot scale horizontally.</p> <p>We run a lot services in VPA auto mode, among them the DCGM exporter.</p>","tags":["kubernetes"]},{"location":"2025-04-09-vpa-flush-historical-data/#roll-out-new-release-of-dcgm-exporter","title":"Roll out new release of DCGM exporter","text":"<p>We already knew from testing that the DCGM exporter had a significant increase in memory resource consumption, so we changed the <code>maxAllowed.memory</code> specification on the <code>VerticalPodAutoscaler</code> custom resource. The hope was that VPA would automatically adjust resources for the DCGM exporter rather quickly, but that didn't happen. DCGM exporter went into OOMKill crashlooping mode while the recommended memory from the VPA slowly crawled upwards. The OOmKill was expected but the slow adjustment from VPA was a surprise. There where probably many contributing factors, but the crashloop backoff didn't help.</p> <p>So how did we solve it?</p>","tags":["kubernetes"]},{"location":"2025-04-09-vpa-flush-historical-data/#flushing-vpa-historical-data","title":"Flushing VPA historical data","text":"<p>In the end we ended up deleting the appropiate <code>VPACheckpoint</code> resource and flushing memory on the VPA recommender component.</p> <pre><code>kubectl delete vpaceckpoint -n dcgm-exporter dcgm-exporter\nkubectl delete pod -n kube-system -l app=vpa-recommender\n</code></pre> <p>This almost immidiatly got the dcgm-exporter to the appropiate resources and out of OOMKill crashlooping.</p>","tags":["kubernetes"]},{"location":"2025-06-12-upgrade-al2023-learnings/","title":"Upgrade from AL2 to AL2023 learnings","text":"<p>Ever since AWS annouced that Amazon Linux 2023 (AL2023) AMI type is replacing Amazon Linux 2 (AL2), I have been excited about it. Mainly because of the cgroup v2 upgrade and the improved security with IMDSv2. To explain it quick</p> <ul> <li>cgroup v2 should provide more transparency when container sub-processes are OOM killed.</li> <li>IMDSv2 will block pods calling the metadata service on the nodes (getting an AWS context) due to a network hop limit.</li> </ul> <p>The AMI upgrade is needed for upgrading worker nodes on EKS from 1.32 to 1.33, since no AL2 AMI is build for 1.33.</p> <p>Upon testing we found a few things breaking, but nothing major. The AWS load balancer controller broke, but only needed the <code>--aws-vpc-id</code> and <code>--aws-region</code> flag set to work again. We ended up removing the spot-termination-exporter (supplying insight into spot-instance interruptions), since it realies heavily on the metadata service, which was now blocked. Sad, but we have lived without it before.</p> <p>We then went on to upgrading all clusters and worker nodes to version 1.33. The upgrade went smooth except for one thing that we overlooked. We rely on flux image-reflector-controller to scan container registries and that also uses the metadata service to use get context of the nodes. Luckily this was a fairly easy fix, where we ended up patching an IRSA role annotation to the image-reflector-controller ServiceAccount in following way.</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - gotk-components.yaml\n  - gotk-sync.yaml\npatches:\n  - patch: |\n      apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        name: image-reflector-controller\n        annotations:\n          eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/eks_CLUSTER_NAME_flux-image-reflector\n    target:\n      kind: ServiceAccount\n      name: image-reflector-controller\n</code></pre> <p>We are now enjoing AL2023 and are so far happy with the upgrade.</p>","tags":["aws","kubernetes"]},{"location":"2025-07-01-ebs-csi-and-al2023/","title":"EBS CSI driver and AL2023","text":"<p>After upgrading to Amazon Linux 2023 (AL2023) we started seeing errors from the aws-ebs-csi-driver running in our clusters.</p> <pre><code>ebs-plugin I0626 06:40:25.662215       1 main.go:154] \"Initializing metadata\"\nebs-plugin I0626 06:40:25.662374       1 metadata.go:66] \"Attempting to retrieve instance metadata from IMDS\"\nebs-plugin E0626 06:40:30.665263       1 metadata.go:72] \"Retrieving IMDS metadata failed\" err=\"could not get IMDS metadata: operation error ec2imds: GetInstanceIdentityDocument, canceled, context deadline exceeded\"\nebs-plugin I0626 06:40:30.665357       1 metadata.go:75] \"Attempting to retrieve instance metadata from Kubernetes API\"\n</code></pre> <p>This is due to AL2023 improved security ensuring features blocking pods from calling metadata service on the nodes due to a network hop limit of 1. The aws-ebs-csi-driver eventually falls back to using the Kubernetes API, but we are waiting ~5 seconds for the call to timeout. With the release of aws-ebs-csi-driver v1.45.0 they have implemented a flag (<code>--metadata-sources</code>) allowing us to set a priority order or choose a specific way of getting metadata. In our case it would be set to <code>\"kubernetes\"</code>.</p> <p>This should prevent above shown errors.</p>","tags":["aws","kubernetes"]},{"location":"2025-07-23-kubeconform/","title":"Client side validations of Kubernetes manifests","text":"<p>To be honest writing Kubernetes manifests can be tedius and it prone to misconfiguration. Of course it will in the end be validated server side, but we would like to avoid most errors before we hand off the manifests to the API server. This can be particular helpful when utilizing GitOps, since the changes will be consumed asynchronous. To achieve this will use the following tooling:</p> <ul> <li>pre-commit</li> <li>kustomize</li> <li>kubeconform</li> <li>CRDs catalog by Datree</li> <li>Github action pre-commit<sup>1</sup></li> </ul> <p>Let's start with <code>kustomize</code> and make sure that we can actually build our manifest bundle.</p> <pre><code>kustomize build path-to-kustomziation-file\n</code></pre> <p>We can now add this to <code>.pre-commit-config.yaml</code> file to the root of the project to have it run every time we commit.</p> <pre><code>repos:\n- repo: local\n  hooks:\n  - id: kustomize\n    name: validate kustmoizations\n    language: system\n    entry: kustomize\n    args:\n    - build\n    - path-to-kustomziation-file\n    always_run: true\n    pass_filenames: false\n</code></pre> <p>Now on to <code>kubeconform</code> for validating our manifests.</p> <pre><code>kubeconform -strict -skip CustomResourceDefinition,Kustomization \\\n  -kubernetes-version 1.33.0 \\\n  -schema-location default \\\n  -schema-location 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json' \\\n  path-to-your-manifests\n</code></pre> <p>We of course depend on the CRDs catalog having our CRs and them being updated, but it is relatively easy to contribute to the catalog see PRs #453 and #600.</p> <p>We can now also add this to our pre-commit config file like so.</p> <pre><code>repos:\n...\n- repo: local\n  hooks:\n  - id: kubeconform\n    name: validate kubernetes manifests\n    language: system\n    entry: kubeconform\n    args:\n    - -strict\n    - -kubernetes-version 1.33.0\n    - -skip\n    - CustomResourceDefinition,Kustomization\n    - -schema-location\n    - default\n    - -schema-location\n    - 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json'\n    files: ^path-to-your-manifests/.*\n</code></pre> <p>Using <code>pre-commit</code> is nice to validate your commits, but it requires everybody to install it and running <code>pre-commit install</code>. So to enforce above validations we can add a CI step in the form of a Github action.</p> <pre><code>name: Pre-commit\non:\n  - pull_request\njobs:\n  pre-commit:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-python@v5\n    - uses: alexellis/arkade-get@master\n      with:\n        kustomize: latest\n        kubeconform: latest\n    - uses: pre-commit/action@v3.0.1\n</code></pre> <p>This setup is not bullet proof, but it do add some extra confidence and it is very low effort to get going.</p> <ol> <li> <p>This action is in maintenance-only mode and you should support the project by using pre-commit.ci instead. But so that everyone can follow the other option is used.\u00a0\u21a9</p> </li> </ol>","tags":["kubernetes"]},{"location":"2025-10-01-kubernetes-resources/","title":"Kubernetes resources","text":"<p>I find my self explaining how I approach setting Kuberentes resources over and over again, and I always struggle rediscovering the good references. So this post serves as a reminder for my self and hopefully it can also help you. I always recommend this 3 part post by Shon Lev-Ran</p> <ul> <li>Kubernetes resources under the hood part 1 (9 min read)</li> <li>Kubernetes resources under the hood part 2 (7 min read)</li> <li>Kubernetes resources under the hood part 3 (9 min read)</li> </ul> <p>But to make it real easy. I always set resource using the following guidelines</p> <ol> <li>Only set resources request for CPU and never set CPU limit.</li> <li>Always set resource request and limit for memory and make sure they are equal.</li> </ol>","tags":["kubernetes"]},{"location":"2025-10-25-automate-crd-updates/","title":"Automate CRD updates","text":"<p>In the previous post about Handle CRDs with GitOps I showed that CRDs for a helm chart can be generated using the <code>helm</code> cli and this enabled us to manage the full lifecycle of CRDs. In this post I will show automation around this, so that a helm chart version update triggers updates to the CRDs aswell.</p>","tags":["kubernetes","gitops","flux","helm","github"]},{"location":"2025-10-25-automate-crd-updates/#requirements","title":"Requirements","text":"<ul> <li>helm</li> <li>yq</li> </ul>","tags":["kubernetes","gitops","flux","helm","github"]},{"location":"2025-10-25-automate-crd-updates/#creating-recipies-for-crd-generation","title":"Creating recipies for CRD generation","text":"<p>Considering our previous example for a <code>HelmRelease</code> for cert-manager</p> <pre><code># helm.yaml\n---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: jetstack\n  namespace: cert-manager\nspec:\n  interval: 15m\n  url: https://charts.jetstack.io\n\n---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: cert-manager\n  namespace: cert-manager\nspec:\n  interval: 5m\n  targetNamespace: cert-manager\n  chart:\n    spec:\n      chart: cert-manager\n      version: \"v1.18.2\"\n      sourceRef:\n        kind: HelmRepository\n        name: jetstack\n      interval: 15m\n  install:\n    crds: Skip\n  values:\n    installCRDs: false\n    ...\n</code></pre> <p>we can create the following recipe for generating CRDs using <code>Makefiles</code> (other technologies can be used)</p> <pre><code>version := $(shell yq '. | select(.kind == \"HelmRelease\") | .spec.chart.spec.version' helm.yaml)\nurl := $(shell yq '. | select(.kind == \"HelmRepository\") | .spec.url' helm.yaml)\nchart := $(shell yq '. | select(.kind == \"HelmRelease\") | .spec.chart.spec.chart' helm.yaml)\nkube_version := v1.22.0 # required &gt;= 1.22.0\nrelease_name := $(shell yq '. | select(.kind == \"HelmRelease\") | .metadata.name' helm.yaml)\n\ncrds.yaml: helm.yaml\n    helm template $(release_name) $(chart) --repo $(url) --version $(version) --set installCRDs=true --kube-version $(kube_version) | yq '. | select(.kind == \"CustomResourceDefinition\")' &gt; $@\n</code></pre> <p>now any updates made to <code>helm.yaml</code> will trigger a generation and overwrite of <code>crds.yaml</code> if <code>make</code> is run.</p>","tags":["kubernetes","gitops","flux","helm","github"]},{"location":"2025-10-25-automate-crd-updates/#creating-a-github-action","title":"Creating a Github action","text":"<p>I have now showed that we can update CRDs manually if our helm chart version changes using <code>make</code>. Now the choice of choosing <code>make</code> makes our life a little bit difficult. In the root of our project I will create one <code>Makefile</code> to call <code>make</code> on all other <code>Makefile</code>'s</p> <pre><code>base_dir := apps # path to our collection of HelmReleases\ncharts_with_crds := $(shell find $(base_dir) -name 'Makefile' -printf \"%h\\n\")\n\nall: $(charts_with_crds)\n\n$(charts_with_crds):\n    @$(MAKE) -C $@\n\n.PHONY: all $(charts_with_crds)\n</code></pre> <p>above <code>Makefile</code> is really complex and wasn't fun to write at all. Let's finish with the Github Action</p> <pre><code># .github/workflows/update-crds.yaml\nname: Update CRDs\non:\n  pull_request:\n    paths:\n      - \"apps/*/**.yaml\"\n      - \"apps/*/**.yml\"\njobs:\n  update-crds:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n      with:\n        ref: ${{ github.event.pull_request.head.ref }}\n\n    - uses: alexellis/arkade-get@master\n      with:\n        helm: latest\n        yq: latest\n\n    - name: Get changed helm files\n      id: changed-files\n      uses: tj-actions/changed-files@v46\n      with:\n        files: |\n          apps/*/helm.yaml\n\n    - name: Touch all changed helm files\n      env:\n        ALL_CHANGED_FILES: ${{ steps.changed-files.outputs.all_changed_files }}\n      run: |\n        for file in ${ALL_CHANGED_FILES}; do\n          echo \"touching file: ${file}\"\n          touch ${file}\n        done\n\n    - name: Update CRDs\n      run: make\n\n    - uses: EndBug/add-and-commit@v9\n      with:\n        add: ./apps/*/crds.yaml\n        message: \"Update CRDs\"\n</code></pre> <p>So now when updates are made to our <code>helm.yaml</code> files updated <code>crds.yaml</code> are commited back by above workflow. The benefit is that changes to CRDs becomes really transparent.</p>","tags":["kubernetes","gitops","flux","helm","github"]},{"location":"2025-10-25-handle-crds-with-gitops/","title":"Handle CRDs with GitOps","text":"<p>In this post we will discuss caveats with full lifecycle management of Custom Resource Definitions (CRDs) with Helm in a GitOps context and give a possible solution.</p>","tags":["kubernetes","gitops","flux","helm"]},{"location":"2025-10-25-handle-crds-with-gitops/#helm-caveats-with-crds-lifecycle-management","title":"Helm caveats with CRDs lifecycle management","text":"<p>Helm is very good at getting CRDs into the cluster at install, but updating and deleting them is where problems tend to arrise. There are solutions in place, like seperate chart for CRDs, but it is very much dependent on how the chart is structured and implemented. Helm documentation has a full section on this here. In summary they write</p> <p>There is no support at this time for upgrading or deleting CRDs using Helm. This was an explicit decision after much community discussion due to the danger for unintentional data loss. Furthermore, there is currently no community consensus around how to handle CRDs and their lifecycle. As this evolves, Helm will add support for those use cases</p> <p>This means that for some charts it might work and for some it might be more challenging, leading to an incoherent experience that could be error prone.</p>","tags":["kubernetes","gitops","flux","helm"]},{"location":"2025-10-25-handle-crds-with-gitops/#a-possible-solution","title":"A possible solution","text":"<p>Seperating handling the CRDs out from Helm and just referencing the manifests directly via kustomize seems like an obvious solution and it works great. In GitOps with FluxCD the <code>HelmRelease</code> has the options <code>.spec.install.crds</code> and <code>.spec.upgrade.crds</code>, the latters default is <code>Skip</code> so we really only have to add it to the install portion. Some charts exposes a <code>installCRDs</code> or similar in its values, so we will also set this to <code>false</code> for clarity even though strictly not nessecary. Let's see an example of the <code>HelmRelease</code> custom resource</p> <pre><code># helm.yaml\n---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: jetstack\n  namespace: cert-manager\nspec:\n  interval: 15m\n  url: https://charts.jetstack.io\n\n---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: cert-manager\n  namespace: cert-manager\nspec:\n  interval: 5m\n  targetNamespace: cert-manager\n  chart:\n    spec:\n      chart: cert-manager\n      version: \"v1.18.2\"\n      sourceRef:\n        kind: HelmRepository\n        name: jetstack\n      interval: 15m\n  install:\n    crds: Skip\n  values:\n    installCRDs: false\n    ...\n</code></pre> <p>and the kustomization file</p> <pre><code># kustomization.yaml\n---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - namespaces.yaml\n  - crds.yaml # the file containing all crds\n  - helm.yaml\n  ...\n</code></pre> <p>The CRDs can normally be generated in one of the two following ways using <code>helm</code> cli</p> <pre><code>helm show crds CHART --repo URL --version VERSION &gt; crds.yaml\n# or\nhelm template RELEASE_NAME CHART --repo URL --version VERSION --set installCRDs=true --kube-version KUBE_VERSION | yq '. | select(.kind == \"CustomResourceDefinition\")' &gt; crds.yaml\n</code></pre> <p>for above cert-manager example the latter option works, but it is rare that it is the complex option of the two. Since we now have documented how to generate the CRDs, automation can be easily added and how I have done it will be show in later post.</p> <p>To quickly conclude. A solution have been presented that manages the full lifecycle (install, update and deletion) of CRDs for helm charts with GitOps assuming pruning of resources is enabled.</p>","tags":["kubernetes","gitops","flux","helm"]},{"location":"2025-11-01-pathing-and-overlays-in-gitops-suck/","title":"Patching and overlays in GitOps suck","text":"<p>Most implementations of Kubernetes clusters that I have come a cross, have migrated to using GitOps as the prefered method of deploying manifests, helm charts and others, and for good reason too. This isn't a rant about how great GitOps is, but rather a a discussion on how to share the same configuration across multiple clusters without extensive patching and lots of overlays.</p>","tags":["kubernetes","gitops","flux"]},{"location":"2025-11-01-pathing-and-overlays-in-gitops-suck/#why-i-dont-like-patching-and-overlays","title":"Why I don't like patching and overlays","text":"<p>If a configuration needs to be shared across many Kubernetes clusters we tend to use the pattern of creating a base configuration and then do individual patches using overlays referencing the base. I think this approach is pretty normal and probably works if you are really strict about what kind of patching you are allowing, but here also lies the pitfall. Patching allows for you to almost do anything to the base configuration, so not only is the interface potentially massive it is also not by definition well defined. You are constantly running the risk of changing something in the base that is patched somewhere in overlays anyway, so all overlays and patches must always be taken into account, probably leading to a small base and a massive overlay. With patching we also need to know the resource Kind, name and maybe namespace plus the yaml path in the spec. But remember we wanted to share as much configuration as possible, so we want as much as possible to go into base.</p> <p>On top of all this I also find patches to be hard to read and not knowing your implementation up front has other drawbacks. I'm not saying that patches and overlays doesn't have their use cases, but limiting them can certainly help.</p>","tags":["kubernetes","gitops","flux"]},{"location":"2025-11-01-pathing-and-overlays-in-gitops-suck/#envsubst-might-be-the-solution","title":"<code>Envsubst</code> might be the solution","text":"<p>If you haven't heard of <code>envsubst</code>, it is a GNU package released in 1995, designed to substitute environment variable references in a given text file or string. In other words we can now parameterize e.g. a Kubernetes manifest. This means that we know where the manifest will change. Time to compare</p> <pre><code># base/sa.yaml for envsubst\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  annotations:\n    eks.amazonaws.com/role-arn: ${KARPENTER_ROLE_ARN} # envsubst notation\n  name: karpenter\n  namespace: karpenter\n</code></pre> <p>and</p> <pre><code># base/sa.yaml for patching\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: karpenter\n  namespace: karpenter\n\n# kustomization\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - ./base/sa.yaml\npatches:\n  - target:\n      kind: ServiceAccount\n      name: karpenter\n      namespace: karpenter\n    patch: |-\n      - op: add\n        path: /metadata/annotations/eks.amazonaws.com~1role-arn\n        value: arn:aws:iam::1234:role/karpenter-role\n</code></pre> <p>The examples might not look so different on the surface and I also did the patching example a disfavor, by not using a placeholder and replacing that. But I wanted to show that from the base configurations point of view there is no knowledge of an annotionation, so if I where to add a similar annotation to the base it would eventually be overruled by the patch eventhough there are no indicators of it being a parameter. Also the overhead of knowing the exact resource and yaml path isn't great. Of course the envsubt example needs to be parsed through the envsubst command <code>KARPENTER_ROLE_ARN=\"arn:aws:iam::1234:role/karpenter-role\" envsubst &lt; base/sa.yaml</code>. But the difference is that my base configuration clearly expects a variable and I do not need to know the resource Kind, name, maybe namespace and yaml path to replace it.</p>","tags":["kubernetes","gitops","flux"]},{"location":"2025-11-01-pathing-and-overlays-in-gitops-suck/#postbuildsubstitution-fluxcd-equivalent-of-envsubst","title":"<code>postBuild.substitution</code> FluxCD equivalent of <code>envsubst</code>","text":"<p>FluxCD (and probably also ArgoCD) has a trick up their sleeve called <code>postBuild.substitution</code> see here, which works like <code>envsubst</code>. Let take above example and try using this feature with the custom resource <code>Kustomization</code> provided by flux</p> <pre><code>---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: apps\nspec:\n  # ...omitted for simplicity\n  postBuild:\n    substitute:\n      KARPENTER_ROLE_ARN: \"arn:aws:iam::1234:role/karpenter-role\"\n</code></pre> <p>We are anyway provisioning above resource if we are doing patching and overlays, so in my mind using post build variable substitution is just simpler and can certainly help minimizing the need for overlays and patching or even remove the need completely. I have run ~10 clusters at the same time all sharing the same configuration only using post build variable substitution and life was just so much simpler.</p>","tags":["kubernetes","gitops","flux"]},{"location":"2025-12-06-grafana-cloud-metrics-cost-savings/","title":"Cost savings efforts for metrics on Grafana Cloud","text":"<p>Grafana Cloud is an appealing option for an observability stack and offloading a selfhosted setup to a managed one gives me peace of mind. I only really have two main complains and it is all related to cost. Metrics and users cost are simply too high, with the metrics one being the worst offender. It forces you to not just enable metrics and be happy, but now you have to do heavy filtering up front on metrics and only keep the ones you think you need. You also have to consider if any of the metrics have a high cardinality label and dropping the labels or use Grafana Cloud's adaptive setup.</p> <p>We have just gone through a week trying to reduced metrics cost and this post will describe our efforts attempted. It is still unclear exactly what the impact on cost will be, but we have seen a huge decline in active series, high cardinality labels and over sampling.</p>","tags":["grafana"]},{"location":"2025-12-06-grafana-cloud-metrics-cost-savings/#sample-rate-savings","title":"Sample rate savings","text":"<p>In our efforts to save cost on metrics we took a look at our sample rates (scrape intervals) and saw that in some cases we had way to high resolution on metrics, 5 seconds in a few worst cases. Our default was 30 seconds, so we decided to be conservative and bump the resolution to 120 seconds. This strikes the perfect balance between enough resolution visualization, alerting and more, without our alerts being too much delayed. This is heavily inspired from learnings done in my old company Veo.</p>","tags":["grafana"]},{"location":"2025-12-06-grafana-cloud-metrics-cost-savings/#filter-unused-metrics","title":"Filter unused metrics","text":"<p>We went through all of our services and controllers and reviewed if the metrics we scraped was actually used for something. This process was time consuming and a bit error prone, but it also had pretty significant effort on active metrics series. This is probably somethings we should have done a long time ago, but never really got the time for it. Here is an example of doing it with a <code>ServiceMonitor</code></p> <pre><code>---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\n...\nspec:\n  endpoints:\n    - path: /metrics\n      port: metrics\n      metricRelabelings:\n        - action: keep\n          sourceLabels: [__name__]\n          regex: \"your_cool_metric_0|your_cool_metrics_1\"\n...\n</code></pre>","tags":["grafana"]},{"location":"2025-12-06-grafana-cloud-metrics-cost-savings/#enable-adaptive-metrics","title":"Enable adaptive metrics","text":"<p>As a last option we enable auto-mode for adaptive metrics, which means that it will automatically adjust the rules over time. This will remove a bunch of labels and most likely also useful ones. But we wanted to reduce cost a lot, so drastic measures where needed. Critically missing labels can always be put back in with excemptions or segments, but this approach could probably quickly become unmanageable in terms of rules exceptions.</p>","tags":["grafana"]},{"location":"2025-12-06-grafana-cloud-metrics-cost-savings/#lacking-cost-impact-feedback","title":"Lacking cost impact feedback","text":"<p>We quickly found out that the way Grafana Cloud does billing dashboards you are kind of flying blind and you are left with proxy dashboards such as active metrics series and sample rate. I will give a short update once we actually realise the savings achieved.</p>","tags":["grafana"]},{"location":"2026-01-10-grafana-cloud-metrics-cost-savings-follow-up/","title":"Cost savings Grafana Cloud follow up","text":"<p>In the previous post I wrote about our efforts reduce cost for Grafana Cloud metrics. Here I went over the 3 main things we implemented</p> <ul> <li>Reduced sample rates</li> <li>Filter/drop unused metrics (keep only used ones)</li> <li>Enable adaptive metrics</li> </ul> <p>but I also ended up concluding that we lacked impact feedback and only had proxy indicators. Our goal was ambitious and more concrete we set out to save 80% on our metrics bill. This post serves as a conclusion on our efforts.</p>","tags":["grafana"]},{"location":"2026-01-10-grafana-cloud-metrics-cost-savings-follow-up/#conclusion","title":"Conclusion","text":"<p>We now know that we almost reached that goal with a 78% reduction in metrics cost alone.</p>","tags":["grafana"]},{"location":"2026-01-10-grafana-cloud-metrics-cost-savings-follow-up/#implementation-aftermath","title":"Implementation aftermath","text":"<p>Enabling auto-mode for adaptive metrics was by far the most invasive and we saw some of the developers dashboards break, but also fewer than antisipated.</p>","tags":["grafana"]},{"location":"2026-01-31-how-this-blog-uses-nix/","title":"How this blog uses Nix","text":"<p>Nix is an advanced tool for building, packaging, and configuring software in a reliable, reproducible and declarative way, that has been gaining a lot of popularity over recent years. Nix first came up on my radar around the early 2020s, but it took a couple of years before I really started investing time on it other than just reading. It is really powerful but also very different from what I was used to. I now use NixOS as my daily driver (work and home) and use Nix Flakes to declare my development shells in various projects. In this post we will go over how I first started using Nix and how I have declared a development shell for this blog using Nix Flakes.</p>","tags":["nix","github"]},{"location":"2026-01-31-how-this-blog-uses-nix/#the-word-nix-is-used-everywhere","title":"The word Nix is used everywhere","text":"<p>The term \"I use Nix\" can have many meanings and is sometimes confusing. Let's go over some of them here.</p> <ul> <li>Nix the functional language</li> <li>Nix the package manager also known as nixpkgs</li> <li>Nix the operating system also known as NixOS</li> </ul> <p>There are probably more, but I think this might illustrate where the confusion comes from. Just know that people tend to only use the word \"Nix\" and you have to guess the context.</p>","tags":["nix","github"]},{"location":"2026-01-31-how-this-blog-uses-nix/#home-manager-is-a-great-place-to-start","title":"Home-manager is a great place to start","text":"<p>I started my practical journey with Nix with porting my dotfiles and packages into the Nix ecosystem using Home-manager, a basic system for managing your user environment using the Nix package manager and Nix libraries. For me it was a great starting point and I can really recommend this approach. At that time I was using Archlinux, but Nix with home-manager could easily be set up on the side and I could slowly port my stuff when I felt like it. I also quickly found out that I almost don't have any system-level configuration, so I made the switch to NixOS after roughly a year and I have never looked back since. See my NixOS configuration here github.com/wcarlsen/config.</p>","tags":["nix","github"]},{"location":"2026-01-31-how-this-blog-uses-nix/#flakes-and-development-shells","title":"Flakes and development shells","text":"<p>Flakes have at this point basically become the defacto standard, when using Nix. It adds a much needed <code>flake.lock</code> file (can be updated with <code>nix flake update</code>), making sure your configuration is reproducable. It is pretty simple to define a development shell using flakes. See look at \"minimal\" example.</p> <pre><code># flake.nix\n{\n  inputs = {\n    nixpkgs.url = \"github:nixos/nixpkgs/nixos-unstable\";\n    flake-utils.url = \"github:numtide/flake-utils\";\n  };\n\n  outputs = inputs:\n    inputs.flake-utils.lib.eachDefaultSystem (system: let\n      pkgs = import inputs.nixpkgs {\n        inherit system;\n      }; # this is just a fancy (but easy) way to define your system, e.g. x86_64-linux, aarch64_darwin, etc.\n    in {\n      devShells = {\n        default = pkgs.mkShell {\n          buildInputs = with pkgs; [\n            cowsay # add your dependencies here\n          ];\n          shellHook = ``\n            cowsay \"COWABUNGA!\" # add your custom shell hooks here\n          ``;\n        };\n      };\n    });\n}\n</code></pre> <p>Above flake consists of <code>inputs</code>, defining which branch of the Nix package manager to use and <code>flake-utils</code> as a way to define systems. The other part is <code>outputs</code>, where we are outputting <code>devShells</code>, but only defining one called <code>default</code> using <code>pkgs.mkShell</code> and its attribute <code>buildInputs</code> to define package dependencies. It should be noted that <code>mkShell</code> has other attributes as well, for example <code>shellHook</code>. You could imagine a simple Python project using UV as package manager, where <code>buildInputs</code> would contain Python and UV and the <code>shellHook</code> running <code>uv sync</code> installing all Python-specific dependencies. Another example would be an Opentofu project, where we install all providers with <code>tofu init</code> in the <code>shellHook</code>.</p> <p>The <code>devShells</code> can be invoked with the following nix command: <code>nix develop</code>. I tend to use <code>direnv</code> and just put <code>use flake</code> in my <code>.envrc</code> file, to have it automatically set up my development shell.</p>","tags":["nix","github"]},{"location":"2026-01-31-how-this-blog-uses-nix/#so-how-does-this-blog-use-nix","title":"So how does this blog use Nix?","text":"<p>Now that we have some limited knowledge about Nix and Flakes, we can start looking at how this blog uses it. In the root of the GitHub project you will find a <code>flake.nix</code> which specifies <code>MkDocs</code> and all the plugins used to create this blog, and, because I use <code>direnv</code>, it will automatically install all dependencies and drop me into a development shell so I can start writing and validate my changes locally. I find the \"holy trinity\" <code>flakes</code>, <code>direnv</code> and <code>make</code> really useful. So now we have a reproducible development setup; how do we use it in places other than locally? Let's look at GitHub Actions as an example.</p>","tags":["nix","github"]},{"location":"2026-01-31-how-this-blog-uses-nix/#github-actions-and-flakes","title":"GitHub Actions and Flakes","text":"<p>Because we have defined all of our dependencies in a <code>Flake</code> it becomes really easy to utilize it in a GitHub Action.</p> <pre><code>name: build\non:\n  pull_request:\n    branches:\n      - main\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Install Nix\n        uses: cachix/install-nix-action@v30\n        with:\n          extra_nix_config: |\n            access-tokens = github.com=${{ secrets.GITHUB_TOKEN }}\n      - name: Build\n        run: nix develop --command make build\n</code></pre> <p>We see that it doesn't really require much effort at all, and changes to my local development don't require updates to my GitHub Actions workflow (unless I change the Makefile interface).</p>","tags":["nix","github"]},{"location":"archive/2026/","title":"2026","text":""},{"location":"archive/2025/","title":"2025","text":""},{"location":"page/2/","title":"Spaghetti \ud83c\udf5d (tech blog)","text":""},{"location":"archive/2025/page/2/","title":"2025","text":""}]}